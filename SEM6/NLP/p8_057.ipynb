{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20BCE057 Devasy Patel\n",
    "\n",
    "# Practical 8\n",
    "\n",
    "## Aim: Generate Word2Vec features on any text corpus / for the example discussed in the class : \n",
    "\n",
    "- (1) without using any library function on a smaller corpus \n",
    "\n",
    "- (2) using Gensim on a larger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: error = 6.734691\n",
      "Epoch 1: error = 6.622298\n",
      "Epoch 2: error = 6.512376\n",
      "Epoch 3: error = 6.408809\n",
      "Epoch 4: error = 6.310009\n",
      "Epoch 5: error = 6.213281\n",
      "Epoch 6: error = 6.120650\n",
      "Epoch 7: error = 6.045642\n",
      "Epoch 8: error = 5.972379\n",
      "Epoch 9: error = 5.900242\n",
      "Epoch 10: error = 5.829212\n",
      "Epoch 11: error = 5.759269\n",
      "Epoch 12: error = 5.690395\n",
      "Epoch 13: error = 5.622571\n",
      "Epoch 14: error = 5.555781\n",
      "Epoch 15: error = 5.490006\n",
      "Epoch 16: error = 5.427776\n",
      "Epoch 17: error = 5.366895\n",
      "Epoch 18: error = 5.311655\n",
      "Epoch 19: error = 5.256549\n",
      "Epoch 20: error = 5.202384\n",
      "Epoch 21: error = 5.152945\n",
      "Epoch 22: error = 5.104102\n",
      "Epoch 23: error = 5.054064\n",
      "Epoch 24: error = 5.012295\n",
      "Epoch 25: error = 4.966256\n",
      "Epoch 26: error = 4.919956\n",
      "Epoch 27: error = 4.880649\n",
      "Epoch 28: error = 4.836448\n",
      "Epoch 29: error = 4.798043\n",
      "Epoch 30: error = 4.763836\n",
      "Epoch 31: error = 4.732407\n",
      "Epoch 32: error = 4.704906\n",
      "Epoch 33: error = 4.673487\n",
      "Epoch 34: error = 4.646832\n",
      "Epoch 35: error = 4.618682\n",
      "Epoch 36: error = 4.589318\n",
      "Epoch 37: error = 4.561981\n",
      "Epoch 38: error = 4.532192\n",
      "Epoch 39: error = 4.507458\n",
      "Epoch 40: error = 4.478686\n",
      "Epoch 41: error = 4.456365\n",
      "Epoch 42: error = 4.433076\n",
      "Epoch 43: error = 4.406865\n",
      "Epoch 44: error = 4.386652\n",
      "Epoch 45: error = 4.365657\n",
      "Epoch 46: error = 4.340756\n",
      "Epoch 47: error = 4.320206\n",
      "Epoch 48: error = 4.299606\n",
      "Epoch 49: error = 4.275289\n",
      "Epoch 50: error = 4.255204\n",
      "Epoch 51: error = 4.234455\n",
      "Epoch 52: error = 4.210983\n",
      "Epoch 53: error = 4.190898\n",
      "Epoch 54: error = 4.171056\n",
      "Epoch 55: error = 4.148318\n",
      "Epoch 56: error = 4.128494\n",
      "Epoch 57: error = 4.105837\n",
      "Epoch 58: error = 4.087185\n",
      "Epoch 59: error = 4.067037\n",
      "Epoch 60: error = 4.044103\n",
      "Epoch 61: error = 4.025775\n",
      "Epoch 62: error = 4.006131\n",
      "Epoch 63: error = 3.983161\n",
      "Epoch 64: error = 3.966058\n",
      "Epoch 65: error = 3.946924\n",
      "Epoch 66: error = 3.927501\n",
      "Epoch 67: error = 3.909715\n",
      "Epoch 68: error = 3.891852\n",
      "Epoch 69: error = 3.872276\n",
      "Epoch 70: error = 3.855632\n",
      "Epoch 71: error = 3.834102\n",
      "Epoch 72: error = 3.818608\n",
      "Epoch 73: error = 3.801023\n",
      "Epoch 74: error = 3.781265\n",
      "Epoch 75: error = 3.766012\n",
      "Epoch 76: error = 3.750179\n",
      "Epoch 77: error = 3.730302\n",
      "Epoch 78: error = 3.714538\n",
      "Epoch 79: error = 3.698890\n",
      "Epoch 80: error = 3.680162\n",
      "Epoch 81: error = 3.664112\n",
      "Epoch 82: error = 3.645571\n",
      "Epoch 83: error = 3.631072\n",
      "Epoch 84: error = 3.614769\n",
      "Epoch 85: error = 3.595832\n",
      "Epoch 86: error = 3.582114\n",
      "Epoch 87: error = 3.566244\n",
      "Epoch 88: error = 3.547821\n",
      "Epoch 89: error = 3.532963\n",
      "Epoch 90: error = 3.518246\n",
      "Epoch 91: error = 3.500849\n",
      "Epoch 92: error = 3.486832\n",
      "Epoch 93: error = 3.468015\n",
      "Epoch 94: error = 3.454536\n",
      "Epoch 95: error = 3.439689\n",
      "Epoch 96: error = 3.421730\n",
      "Epoch 97: error = 3.408068\n",
      "Epoch 98: error = 3.393460\n",
      "Epoch 99: error = 3.376215\n",
      "over: [-0.38015994  0.4629961  -0.49959703 -0.14929666 -0.32222001]\n",
      "climbed: [0.4452038  0.45550263 0.07258904 0.2389712  0.08737344]\n",
      "jumped: [-0.46144563  0.2972183  -0.39984374  0.16189053 -0.30024796]\n",
      "quick: [ 0.32381915  0.2828736   0.42764455 -0.32428599 -0.17691317]\n",
      "the: [ 0.02057868 -0.38139024  0.39585519 -0.11593609 -0.00778148]\n",
      "brown: [-0.3400725   0.13883869  0.32857712  0.38892562 -0.42182229]\n",
      "hate: [ 0.35962348  0.36331115  0.10090743 -0.3735923   0.2360573 ]\n",
      "cat: [-0.27647684  0.33099495  0.22280244  0.33180324  0.24691249]\n",
      "fox: [-0.3887566  -0.46255544  0.35527619  0.01011311  0.06518295]\n",
      "other: [-0.31836383 -0.17676637 -0.34192415 -0.41482799 -0.39479982]\n",
      "chased: [-0.05691081 -0.29147276 -0.0971807  -0.20322936  0.47030715]\n",
      "each: [-0.26465057  0.16228467  0.49939965 -0.35718358  0.20916862]\n",
      "and: [-0.46512566 -0.14951873 -0.02050443 -0.20778584  0.22141647]\n",
      "tree: [ 0.25293579  0.05558997  0.49740167 -0.43463821 -0.40665646]\n",
      "lazy: [-0.05312704  0.3061555  -0.35238224  0.08313976 -0.15895985]\n",
      "dog: [-0.38466833  0.39352565  0.14075354  0.11716386  0.48679174]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define a small text corpus\n",
    "corpus = [    \"the quick brown fox\",    \"jumped over the lazy dog\",    \"the dog chased the cat\",    \"the cat climbed the tree\", \"dog and cat hate each other\"]\n",
    "\n",
    "# Define the hyperparameters of the ANN\n",
    "embedding_size = 5\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Create a vocabulary from the text corpus\n",
    "vocabulary = set()\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        vocabulary.add(word)\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# Initialize the weights of the ANN\n",
    "input_weights = np.random.uniform(-0.5, 0.5, size=(vocabulary_size, embedding_size))\n",
    "output_weights = np.random.uniform(-0.5, 0.5, size=(embedding_size, vocabulary_size))\n",
    "\n",
    "# Train the ANN on the text corpus\n",
    "for epoch in range(epochs):\n",
    "    error = 0\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words)):\n",
    "            input_word = np.zeros((vocabulary_size, 1))\n",
    "            input_word[list(vocabulary).index(words[i])] = 1\n",
    "            output_word = np.zeros((vocabulary_size, 1))\n",
    "            if i < len(words) - 1:\n",
    "                output_word[list(vocabulary).index(words[i + 1])] = 1\n",
    "            input_layer = input_weights.T.dot(input_word)\n",
    "            hidden_layer = input_layer\n",
    "            output_layer = output_weights.T.dot(hidden_layer)\n",
    "            predicted_word = np.argmax(output_layer)\n",
    "            target_word = np.argmax(output_word)\n",
    "            error += abs(output_layer[predicted_word] - output_layer[target_word])\n",
    "            output_weights[:, predicted_word] -= learning_rate * (output_layer[predicted_word] - output_layer[target_word]) * hidden_layer[:, 0]\n",
    "    print(\"Epoch %d: error = %f\" % (epoch, error))\n",
    "\n",
    "# Get the word embeddings from the ANN\n",
    "word_embeddings = input_weights\n",
    "\n",
    "# Print the word embeddings\n",
    "for i, word in enumerate(vocabulary):\n",
    "    print(\"%s: %s\" % (word, str(word_embeddings[i])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=1, vector_size=100, alpha=0.025>\n",
      "['sentence']\n",
      "[[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      "  -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      "  -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      "  -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "   2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "   7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "   6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      "  -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "   9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "   8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      "  -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      "  -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "   4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      "  -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "   4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      "  -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      "  -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      "  -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      "  -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "   7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      "  -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      "  -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      "  -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "   3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      "  -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]]\n",
      "Word2Vec<vocab=1, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, window=5, min_count=5, workers=4)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.index_to_key)\n",
    "# words = list(model.wv.vocab.keys())\n",
    "print(words)\n",
    "# access vector for one word \n",
    "\n",
    "\n",
    "# save model\n",
    "print(model.wv.vectors)\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
