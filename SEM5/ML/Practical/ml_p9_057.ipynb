{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  0.3292953156205685\n",
      "prediction:  [[4.99116965e-08]\n",
      " [7.67757434e-09]\n",
      " [1.43295916e-08]\n",
      " [2.61755184e-09]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "# implement exor gate using backpropagation neural network from scratch\n",
    "# using sigmoid activation function\n",
    "\n",
    "xt = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "yt = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# initialize weights\n",
    "w1 = np.random.rand(2,2)\n",
    "w2 = np.random.rand(2,1)\n",
    "\n",
    "# initialize bias\n",
    "b1 = np.random.rand(2)\n",
    "b2 = np.random.rand(1)\n",
    "\n",
    "# define sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# define derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    " \n",
    "# define forward propagation\n",
    "def forward_propagation(x):\n",
    "    z1 = np.dot(x,w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1,w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    return z1,a1,z2,a2\n",
    " \n",
    "# define backward propagation\n",
    "def backward_propagation(x,y,z1,a1,z2,a2):\n",
    "    dz2 = a2 - y\n",
    "    dw2 = np.dot(a1.T,dz2)\n",
    "    db2 = np.sum(dz2,axis=0,keepdims=True)\n",
    "    dz1 = np.dot(dz2,w2.T)*sigmoid_derivative(z1)\n",
    "    dw1 = np.dot(x.T,dz1)\n",
    "    db1 = np.sum(dz1,axis=0)\n",
    "    return dz2,dw2,db2,dz1,dw1,db1\n",
    "\n",
    "\n",
    "# define gradient descent\n",
    "def gradient_descent(x,y,epochs,lr, w1, w2, b1, b2):\n",
    "    for i in range(epochs):\n",
    "        z1,a1,z2,a2 = forward_propagation(x)\n",
    "        dz2,dw2,db2,dz1,dw1,db1 = backward_propagation(x,y,z1,a1,z2,a2)\n",
    "        w1 = w1 - lr*dw1\n",
    "        b1 = b1 - lr*db1\n",
    "        w2 = w2 - lr*dw2\n",
    "        b2 = b2 - lr*db2\n",
    "        if i%1000==0:\n",
    "            print(\"cost: \",np.mean(np.square(y-a2)))\n",
    "    return w1,b1,w2,b2\n",
    "\n",
    "# define predict function\n",
    "def predict(x):\n",
    "    z1,a1,z2,a2 = forward_propagation(x)\n",
    "    return a2\n",
    "\n",
    "# train the model\n",
    "w1,b1,w2,b2 = gradient_descent(xt,yt,1000,0.01, w1, w2, b1, b2)\n",
    " \n",
    "# predict\n",
    "print(\"prediction: \",predict(xt))\n",
    "  \n",
    "# print accuracy\n",
    "\n",
    "# print(\"accuracy: \",sklearn.metrics.accuracy_score(yt,np.round(predict(xt))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2ed1c1e3dda2d1d09010f3df351d528d97a393444344645f0fddd23da56b409"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
